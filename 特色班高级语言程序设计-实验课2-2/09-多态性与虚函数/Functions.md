# 虚函数：实现自动求导算子基类Function

自动求导模块是现代深度学习框架的一个重要部分，它是实现反向传播算法的关键技术，其推导效率和计算效率直接决定了深度学习框架的训练和推理效率。

**反向传播算法**：反向传播算法是神经网络中的一种重要的学习算法，其主要目标是计算神经网络中权值的梯度。BP算法的关键步骤如下：

1. **前向传播**：输入样本，通过神经网络进行计算，得到预测值。
2. **计算损失**：使用损失函数（如均方误差）计算预测值与真实值之间的差距。
3. **反向传播**：计算损失函数关于每个权重和偏置的梯度。这是通过链式法则来完成的。

**链式求导**是反向传播中计算梯度的关键步骤。假设我们有一个复合函数$f(g(x))$，我们想要计算这个函数关于$x$的导数。根据链式法则，我们有：

$$\frac{df}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}$$

在神经网络中，我们通常有多个这样的复合函数，因此我们需要反复应用链式法则来计算梯度。
例如，假设我们的损失函数为$L = f(y, t)$，其中$y = g(x, w)$是神经网络的输出，$t$是目标值，$w$是权重。我们想要计算损失函数关于权重的梯度，即$\frac{dL}{dw}$。根据链式法则，我们有：

$$\frac{dL}{dw} = \frac{dL}{dy} \cdot \frac{dy}{dw}$$

这就是BP算法中链式求导的基本原理。

**自动求导模块**：自动求导模块是现代深度学习框架的一个重要的组成部分，其通过接收来自上层的梯度（比如上面提到的$\frac{dL}{dy}$），以及为每个基本算子计算当前层的梯度（比如上面提到的 $\frac{dy}{dw}$），来自动计算输出对算子每一个输入的梯度，并且通过逐步地将梯度往下层传播，可以实现任意计算图的自动求导。

对于自动求导模块的关键模块包括：

1. 一个用于自动求导的算子基类Function，Function类是一个抽象基类，其具有forward和backward两组纯虚函数（可能有多种不同参数表的重载函数），以及一个ctx的上下文对象列表，类型为vector<GradTensor>，
2. 若干个继承自Function的派生类，其根据实际计算规则实现forward和backward函数用于计算前向和反向传播。其中forward计算函数的输出结果，backward则由输出的梯度计算输入的梯度。

对于计算公式是$Z=f(Y)$, $Y=H(A)+A$, $H(A)=A*A$，我们需要实现+和*（对位乘法）两个操作的forward和backward函数。其中forward函数就是由两个输入计算得到一个输出结果（+或者*）

我们假设$\frac{dZ}{dY}=K$是已知的，按照以下步骤计算：

1. 在+运算的backward函数需要接受一个梯度值$\frac{dZ}{dY}$作为参数，然后来分别计算两个输入$H(A)$和$A$的梯度，即：$\frac{dZ}{d(H(A))}=\frac{dZ}{dY}\cdot\frac{dY}{d(H(A))}$ 以及 $\frac{dZ}{d(A)}=\frac{dZ}{dY}\cdot\frac{dY}{dA}$。 值得注意的是我们在一个运算中不展开计算底层的梯度，即$H(A)$对$A$的梯度，因为这个将会在链式求导的下一步再计算。$\frac{dZ}{dY}=K$是已知的。
2. 接下来，我们需要再计算的是*运算的输出对两个输入的梯度，在计算公式 $H(A)=A*A$中我们知道输出是$H(A)$，输入是$A$和$A$（为了通用性，此处我们看作两个参数分别求导），则对前一个$A$求梯度为：$\frac{dZ}{dA}=\frac{dZ}{d(H(A))}\cdot\frac{d(H(A))}{dA}$, 对后一个$A$求梯度也是：$\frac{dZ}{dA}=\frac{dZ}{d(H(A))}\cdot\frac{d(H(A))}{dA}$, 注意这里的$\frac{dZ}{d(H(A))}$是我们在上一步已经求出来的。
3. 最后，我们将所有步骤中求得的$\frac{dZ}{dA}$加起来得到总的$\frac{dZ}{dA}$的梯度，即：$\frac{dZ}{dY}\cdot\frac{dY}{dA}$（步骤1得出的）+$\frac{dZ}{d(H(A))}\cdot\frac{d(H(A))}{dA}$+$\frac{dZ}{d(H(A))}\cdot\frac{d(H(A))}{dA}$=$(K*1) + (K*1*A)+(K*1*A)$. 请注意这三个结果分别是在不同的步骤中计算获得的。